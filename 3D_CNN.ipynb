{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5xvMtMM6f1Z"
      },
      "source": [
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
        "from keras.layers import Dropout, Input, BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adadelta\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib.pyplot import cm\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import keras\n",
        "import h5py\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline\n",
        "\n",
        "# file contains data set -- update ###############\n",
        "with h5py.File('', 'r') as dataset: \n",
        "    x_train = dataset[\"X_train\"][:]\n",
        "    x_test = dataset[\"X_test\"][:]\n",
        "    y_train = dataset[\"y_train\"][:]\n",
        "    y_test = dataset[\"y_test\"][:]\n",
        "\n",
        "\n",
        "print (\"x_train shape: \", x_train.shape)\n",
        "print (\"y_train shape: \", y_train.shape)\n",
        "\n",
        "print (\"x_test shape:  \", x_test.shape)\n",
        "print (\"y_test shape:  \", y_test.shape)\n",
        "\n",
        "\n",
        "## Introduce the channel dimention in the input dataset \n",
        "# 4 dimensions for 3d cnn -- change values ###############\n",
        "xtrain = np.ndarray(())\n",
        "xtest = np.ndarray(())\n",
        "\n",
        "\n",
        "## convert to 1 + 4D space (1st argument represents number of rows in the dataset) -- change values ###############\n",
        "xtrain = xtrain.reshape(x_train.shape[0], 16, 16, 16, 3)\n",
        "xtest = xtest.reshape(x_test.shape[0], 16, 16, 16, 3)\n",
        "\n",
        "## convert target variable into one-hot\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "y_train.shape\n",
        "\n",
        "\n",
        "## input layer\n",
        "input_layer = Input((16, 16, 16, 3))\n",
        "\n",
        "## convolutional layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
        "\n",
        "## add max pooling to obtain the most imformatic features\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
        "\n",
        "## perform batch normalization on the convolution outputs before feeding it to MLP architecture\n",
        "pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "\n",
        "## create an MLP architecture with dense layers : 4096 -> 512 -> 10\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=2048, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=512, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer = Dense(units=10, activation='softmax')(dense_layer2)\n",
        "\n",
        "## define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=Adadelta(lr=0.1), metrics=['acc'])\n",
        "model.fit(x=xtrain, y=y_train, batch_size=128, epochs=50, validation_split=0.2)\n",
        "\n",
        "\n",
        "pred = model.predict(xtest)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}